{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine tune Llama 2","metadata":{}},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7","metadata":{"execution":{"iopub.status.busy":"2024-04-22T08:35:30.225297Z","iopub.execute_input":"2024-04-22T08:35:30.226145Z","iopub.status.idle":"2024-04-22T08:35:42.815282Z","shell.execute_reply.started":"2024-04-22T08:35:30.226108Z","shell.execute_reply":"2024-04-22T08:35:42.814092Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Get the number of available GPUs\nnum_gpus = torch.cuda.device_count()\n\nif num_gpus > 0:\n    print(\"Available GPUs:\")\n    for i in range(num_gpus):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\nelse:\n    print(\"No GPUs available.\")\n\ndevice_ids = list(range(num_gpus))\ndevice_ids","metadata":{"execution":{"iopub.status.busy":"2024-04-22T08:35:42.817302Z","iopub.execute_input":"2024-04-22T08:35:42.817601Z","iopub.status.idle":"2024-04-22T08:35:45.169119Z","shell.execute_reply.started":"2024-04-22T08:35:42.817575Z","shell.execute_reply":"2024-04-22T08:35:45.168199Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Available GPUs:\nGPU 0: Tesla T4\nGPU 1: Tesla T4\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"[0, 1]"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-04-22T08:35:45.170630Z","iopub.execute_input":"2024-04-22T08:35:45.171096Z","iopub.status.idle":"2024-04-22T08:35:54.063109Z","shell.execute_reply.started":"2024-04-22T08:35:45.171069Z","shell.execute_reply":"2024-04-22T08:35:54.062123Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-04-22 08:35:47.922766: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-22 08:35:47.922828: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-22 08:35:47.924330: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nhuggingface_token = UserSecretsClient().get_secret(\"huggingface_token\")\n\nfrom huggingface_hub import login\n\nlogin(token=huggingface_token)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T08:35:54.065250Z","iopub.execute_input":"2024-04-22T08:35:54.065542Z","iopub.status.idle":"2024-04-22T08:35:54.307604Z","shell.execute_reply.started":"2024-04-22T08:35:54.065518Z","shell.execute_reply":"2024-04-22T08:35:54.306745Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**How to fine tune Llama 2**\n* a 15GB Graphics Card (Limited Resources --> Barely enough to store Llama 2–7b’s weights)\n* We also need to consider the overhead due to optimizer states, gradients, and forward activations\n* Full fine-tuning is not possible here: we need parameter-efficient fine-tuning (PEFT) techniques like LoRA or QLoRA.\n* To drastically reduce the VRAM usage, we must fine-tune the model in 4-bit precision, which is why we’ll use QLoRA here.","metadata":{}},{"cell_type":"markdown","source":"**Load dataset**","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset('vishnun0027/guanaco-llama2', split=\"train[:1000]\")\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-04-22T08:35:54.308831Z","iopub.execute_input":"2024-04-22T08:35:54.309160Z","iopub.status.idle":"2024-04-22T08:35:55.773408Z","shell.execute_reply.started":"2024-04-22T08:35:54.309134Z","shell.execute_reply":"2024-04-22T08:35:55.772418Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text'],\n    num_rows: 1000\n})"},"metadata":{}}]},{"cell_type":"code","source":"compute_dtype = getattr(torch,'float16')\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=False,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and True:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T08:35:55.774705Z","iopub.execute_input":"2024-04-22T08:35:55.775079Z","iopub.status.idle":"2024-04-22T08:35:55.782904Z","shell.execute_reply.started":"2024-04-22T08:35:55.775046Z","shell.execute_reply":"2024-04-22T08:35:55.782178Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Load base model\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"NousResearch/Llama-2-7b-chat-hf\",\n    quantization_config=bnb_config,\n    device_map={\"\": 0}\n)\n# Wrap the model with DataParallel\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"execution":{"iopub.status.busy":"2024-04-22T08:35:55.784045Z","iopub.execute_input":"2024-04-22T08:35:55.784329Z","iopub.status.idle":"2024-04-22T08:36:01.601724Z","shell.execute_reply.started":"2024-04-22T08:35:55.784305Z","shell.execute_reply":"2024-04-22T08:36:01.600941Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63b2e5e2521e48f58966eab89a316f8d"}},"metadata":{}}]},{"cell_type":"code","source":"# # Load base model\n# from torch.nn.parallel import DataParallel\n\n# model = AutoModelForCausalLM.from_pretrained(\n#     \"NousResearch/Llama-2-7b-chat-hf\",\n#     quantization_config=bnb_config,\n# )\n# # Wrap the model with DataParallel\n# model = DataParallel(model, device_ids=device_ids)  \n# model.config.use_cache = False\n# model.config.pretraining_tp = 1","metadata":{"execution":{"iopub.status.busy":"2024-04-22T08:36:01.602861Z","iopub.execute_input":"2024-04-22T08:36:01.603175Z","iopub.status.idle":"2024-04-22T08:36:01.607916Z","shell.execute_reply.started":"2024-04-22T08:36:01.603150Z","shell.execute_reply":"2024-04-22T08:36:01.606680Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\", trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T08:36:01.609596Z","iopub.execute_input":"2024-04-22T08:36:01.609943Z","iopub.status.idle":"2024-04-22T08:36:01.850019Z","shell.execute_reply.started":"2024-04-22T08:36:01.609913Z","shell.execute_reply":"2024-04-22T08:36:01.849247Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T08:36:01.853648Z","iopub.execute_input":"2024-04-22T08:36:01.854062Z","iopub.status.idle":"2024-04-22T08:36:01.858574Z","shell.execute_reply.started":"2024-04-22T08:36:01.854037Z","shell.execute_reply":"2024-04-22T08:36:01.857618Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train**","metadata":{}},{"cell_type":"code","source":"# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=\"results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    save_steps=0,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay= 0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type='cosine',\n    report_to=\"tensorboard\",\n#     push_to_hub=True,\n    \n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=None,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n)\n\n# Train model\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T08:36:01.859927Z","iopub.execute_input":"2024-04-22T08:36:01.860618Z","iopub.status.idle":"2024-04-22T09:00:49.627203Z","shell.execute_reply.started":"2024-04-22T08:36:01.860584Z","shell.execute_reply":"2024-04-22T09:00:49.626360Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd0b67e768c747d49807fdbd98b18bde"}},"metadata":{}},{"name":"stderr","text":"You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [31/31 22:44, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.738200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=31, training_loss=1.746327369443832, metrics={'train_runtime': 1474.9984, 'train_samples_per_second': 0.678, 'train_steps_per_second': 0.021, 'total_flos': 8134444407521280.0, 'train_loss': 1.746327369443832, 'epoch': 0.99})"},"metadata":{}}]},{"cell_type":"code","source":"# Save trained model\ntrainer.model.save_pretrained(\"Llama-2-7b-chat-finetune\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:00:49.628363Z","iopub.execute_input":"2024-04-22T09:00:49.628670Z","iopub.status.idle":"2024-04-22T09:00:49.835266Z","shell.execute_reply.started":"2024-04-22T09:00:49.628645Z","shell.execute_reply":"2024-04-22T09:00:49.834372Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir results/runs","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:00:49.836491Z","iopub.execute_input":"2024-04-22T09:00:49.836846Z","iopub.status.idle":"2024-04-22T09:00:56.371142Z","shell.execute_reply.started":"2024-04-22T09:00:49.836814Z","shell.execute_reply":"2024-04-22T09:00:56.369938Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]},{"cell_type":"code","source":"# Ignore warnings\nlogging.set_verbosity(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"What is a large language model?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:00:56.372458Z","iopub.execute_input":"2024-04-22T09:00:56.372806Z","iopub.status.idle":"2024-04-22T09:01:51.546537Z","shell.execute_reply.started":"2024-04-22T09:00:56.372777Z","shell.execute_reply":"2024-04-22T09:01:51.545546Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"<s>[INST] What is a large language model? [/INST]  A large language model is a type of artificial intelligence (AI) model that is trained on a large dataset of text to generate language outputs that are coherent and natural-sounding. everybody has a unique personality, and the same is true for language models. Large language models are trained on vast amounts of text data, which allows them to learn patterns and relationships in language.\n\nLarge language models are trained on vast amounts of text data, which allows them to learn patterns and relationships in language. They are capable of generating text that is coherent and natural-sounding, and they can be used for a wide range of applications, such as language translation, text summarization, and chatbots.\n\nLarge language models are trained on vast amounts of text data, which allows them to learn patterns and relationships in language. They are capable of generating text that is coherent\n","output_type":"stream"}]},{"cell_type":"code","source":"# Empty VRAM\ndel model\ndel pipe\ndel trainer\nimport gc\ngc.collect()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:01:51.547605Z","iopub.execute_input":"2024-04-22T09:01:51.547896Z","iopub.status.idle":"2024-04-22T09:01:52.156092Z","shell.execute_reply.started":"2024-04-22T09:01:51.547863Z","shell.execute_reply":"2024-04-22T09:01:52.155180Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"****Store New Llama2 Model (Llama-2-7b-chat-finetune)****\n\n*How can we store our new Llama-2-7b-chat-finetune model now? We need to merge the weights from LoRA with the base model. Unfortunately, as far as I know, there is no straightforward way to do it: we need to reload the base model in FP16 precision and use the peft library to merge everything.*","metadata":{}},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"NousResearch/Llama-2-7b-chat-hf\",\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},\n)\nmodel = PeftModel.from_pretrained(base_model, 'Llama-2-7b-chat-finetune')\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained('NousResearch/Llama-2-7b-chat-hf', trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:13:16.919116Z","iopub.execute_input":"2024-04-22T09:13:16.919779Z","iopub.status.idle":"2024-04-22T09:13:54.696006Z","shell.execute_reply.started":"2024-04-22T09:13:16.919745Z","shell.execute_reply":"2024-04-22T09:13:54.695050Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b17fc4dc74f46cd93d4ed47efd01ead"}},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(\"vishnun0027/Llama-2-7b-chat-finetune\")\n\ntokenizer.push_to_hub(\"vishnun0027/Llama-2-7b-chat-finetune\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:13:54.697701Z","iopub.execute_input":"2024-04-22T09:13:54.698086Z","iopub.status.idle":"2024-04-22T09:18:54.869402Z","shell.execute_reply.started":"2024-04-22T09:13:54.698054Z","shell.execute_reply":"2024-04-22T09:18:54.868391Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ea5c9a39eb74599bb75540c2bbce945"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a39895622124abd91ad620e8a65e4d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09e24a1c8a5b4afabe21b63e0b981b90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"435f93bd671f4a18ad6a0db0190932e7"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/vishnun0027/Llama-2-7b-chat-finetune/commit/d7376db1e15a1ff68aa0275483d68e829ae3e8f8', commit_message='Upload tokenizer', commit_description='', oid='d7376db1e15a1ff68aa0275483d68e829ae3e8f8', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}